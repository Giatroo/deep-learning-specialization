%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt, a4paper, oneside]{book}

\input{/home/giatro/.config/user/giatro_packages.tex}

\input{/home/giatro/.config/user/giatro_macros.tex}

\title{Deep Learning\\\small{by DeepLearning.AI}}
\date{\today}
\author{Lucas Paiolla Forastiere}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}%
\label{cha:introduction}

The term deep learning refers to training \textit{neural networks}, sometimes
very big neural networks. But what are neural networks?

So let's suppose we want to predict housing prices based on the size of the
house. And let's say we'll use Logistic Regression to do that. But as we know,
house prices can't be negative, so we simply say the value of the house is $0$
if the Logistic Regression would predict something negative.

That's indeed the simplest neural network we can have, we have a single input
\texttt{size} and a single output \texttt{price} and in the middle we have a
single neuron: the logistic regression.

\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{Res/housing_logistic_regression.png}
\caption{Here we see the graph of the problem we described.}
\label{housing_logistic_regression.png}
\end{figure}

That function which is zero and than linear is called \textit{ReLU} and it's
used a lot in neural networks. It stands for \textit{Rectified Linear Unit}.

So to get a bigger neural network, we stack these neurons. Instead of predicting
using only the size of house, we could use the number of bedrooms, zip code and
wealth. We could use the size and number of bedrooms to predict the family size;
use the zip code to predict the walkability; and use the zip code and wealth to
predict the school quality. And then, we could use the family size, walkability
and school quality to predict the price. See in the picture:

\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{Res/housing_nn.png}
\caption{Now we have a more complex neural network, which is the stack of many
ReLUs.}
\label{/housing_nn.png}
\end{figure}

However, in general what we have is something a little more complex than that.
We would have something like figure \ref{nn_generic.png}. Here we see that the
internal nodes (which are called \textbf{hidden nodes} or \textbf{hidden
neurons} or \textbf{hidden units}) receive the output of all the previous nodes
to make it predictions. These hidden nodes don't really have a meaning like the
example we gave. We don't try to predict family size or walkability or whatever,
we simply let the neural network decide what that neuron will output in order to
predict the final output \texttt{price} in the better way it can.

\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{Res/nn_generic.png}
\caption{The generic form of a neural network.}
\label{nn_generic.png}
\end{figure}

We can use neural networks in many applications, here we're going to foucos in
\textbf{supervised learning}, which are problems that you have a set of
variables called input (represented by $x$) and an output ($y$) related to that
input. In order to solve these kind of problems, there are many kinds of neural
networks. The one we saw is the most common one, but there are others, like
convolutional nn or recurrent nn.

\begin{figure}[h]
\centering
\includegraphics[scale=0.3]{Res/examples_nn.png}
\caption{Examples of neural networks.}
\label{examples_nn.png}
\end{figure}

Another thing that's important to decide what kind of nn we'll use is knowing if
the data we're leading with is \textit{structured} or \textit{unstructured}.

\textbf{Structured data} is data in the form of a table. We have a very clear
set of input variable $X$ and a set of output variables $y$. Each line of our
table represents one instance of data with many inputs and one or more outputs
related to those inputs.

\textbf{Unstructured data} is all the other kinds of data: audio, video, texts,
images, etc.

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{Res/structured_vs_unstructed_data.png}
\caption{The two kinds of data.}
\label{structured_vs_unstructed_data.png}
\end{figure}

It turns out that machine learning algorithms performed better on structured
data over the years and more recently neural networks are performing better also
on unstructured data.

\paragraph{Why is Deep Learning taking off?}%
\label{par:why_is_deep_learning_taking_off_}

This is one of the questions we must ask ourselves when begining to learn deep
learning. Let's see the graph of the performance of the machine learning
algorithms versus the amount of data that we provide to then. We see that
traditional learning algorithms have a plato where they can't improve anymore,
which neural networks can lead with that data as we make than bigger and bigger.

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{Res/ml_algorithms_performance.png}
\caption{The performance of machine learning algorithms in respect to the data
we provide to them.}
\label{ml_algorithms_performance.png}
\end{figure}

We also see in the graph that when we don't have a large amount of data, NNs
all algorithms perform pretty much the same.

So in order to answer our question, we have to understand the evolution of three
things: \textit{data}, \textit{computation} and \textit{algorithms}.

Through the years, the amount of data available was inscreased a lot, so NNs can
take advantage from that. Also the computation power was inscreased with the use
of GPUs to make a large amount of computations. And finally new algorithms have
been developed to make NNs faster. That's the main reason why deep learning is
taking off.

\section{Notation}%
\label{sec:notation}

Before continuing, we need to define the notation we're going to use.

\begin{itemize}
    \item $(x,y)$ will denote a single training input;
    \item $m$ or $m_{\text{train}}$ denotes the number of training examples;
    \item $\xii[x]{i}$ denotes the $i$-th training input;
    \item $\xii[y]{i}$ denotes the $i$-th traning output;
    \item $n_x$ or $n$ denotes the number of dimensions $x$ has (or the number
        of features);
    \item $m_{\text{test}}$ denotes the number of testing examples;
    \item $X$ is the matrix of all traning examples. It's defined as:
        \[
        X = \begin{bmatrix}
            | & | &  & | \\
            \xii[x]{1} & \xii[x]{2} & \cdots & \xii[x]{m} \\
            | & | &  & | \\
        \end{bmatrix}
        \]
        $X$ is an $m\times n$ matrix;
    \item $Y$ is the matrix of all outputs. It's defined as:
        \[
        Y = \begin{bmatrix}
            \xii[y]{1} & \xii[y]{2} & \cdots & \xii[y]{m}
        \end{bmatrix}
        \]
        $Y$ is a $1\times m$ matrix.
\end{itemize}

\begin{obs}
In other courses we might see $X$ defined as the transpouse of the matrix we've
just defined. But it turns out that when using this definiting, it's much easier
to implement algorithms, so remember te definition we're going to use through out
the course.

The same thing for $Y$. We see that here $Y$ is the transpouse of that it's
tends to be in other courses.
\end{obs}

\section{Logistic Regression as a Neural Network}%
\label{par:logistic_regression_as_a_neural_network}

To end this introduction, we'll see the basics of neural network programming
using the simplest NN we can: a logistic regression.

So let's recall what's logistic regression and why it's useful. Logistic
Regression is used in binary classification, the kind of problem where we have
an input and want to predict between $0$ or $1$. An example could be an image
and we want to say it what's a cat ($1$) or not ($0$).

Basicly we want an algorhthm to estimate the probability of $y=1$ given $x$. In
math we write:
\[
    \hat{y}=P(y=1\cond x),\t\t x\in\R^{n}
\]

Logistic Regression estimates this quantity using the formula:

\[
    \hat{y}=\sigma(w^{T}x+b),
\]
where $w$ and $b$ are parameters to be discovered and $\sigma$ is the
\textbf{sigmoid function}:

\[
    \sigma(z) = \dfrac{1}{1+e^{-z}}
\]

\begin{figure}[h]
\centering
\includegraphics[scale=0.7]{Res/sigmoid.png}
\caption{A sigmoid graph.}
\label{sigmoid.png}
\end{figure}

It's also common to create a new input $x_0=1$ and use the $x$ vector as
$x\in\R^{n+1}$ and use the formula $\hat{y}=\sigma(\theta^{T}x)$, where

\[
\theta=\begin{bmatrix}
    \theta_0 \\ \theta_1 \\ \theta_2 \\ \vdots \\ \theta_n
\end{bmatrix}\t \theta_0=b\t w=\begin{bmatrix}
    \theta_1 \\ \theta_2 \\ \vdots \\ \theta_n
\end{bmatrix}
\]

To find the parameters $b$ and $w$, we need to define a \textbf{cost function},
which is a function that says how badly our algorithm is performing. This is a
function that we want to minimize and when we minimize, we find the best values
of $b$ and $w$.

The \textbf{cost} function is a function of all training examples, while a
\textbf{loss function} or \textbf{error function} is a function of a single
traning example that measures how well our algorithm is performing.

For logistic regression, we use the loss function:

\[
    \mathcal{L}(\hat{y},y)=-y \log\hat{y}-(1-y)\log(1-\hat{y})
\]

Notice that this is the same as:
\[
    \mathcal{L}(\hat{y},y)=\begin{cases}
        -\log(\hat{y}-1), &\text{ if } y=0\\
        -\log\hat{y}, &\text{ if } y=1\\
    \end{cases}
\]

That give us the cost function:
\[
    J(w,b)=\dfrac{1}{m}\spsum{i=1}{m}\mathcal{L}(\xii[\hat{y}]{i},\xii[y]{i})
\]

\subsection{Gradient Descent}%
\label{sub:gradient_descent}

We know have:
\begin{itemize}
    \item A way of predicting the classes $0$ or $1$ using the sigmoid function;
    \item A way of measuring the error of our predictions.
\end{itemize}

What we need now is a way of chaning our parameters $b$ and $w$ in order to
minimize the error. That's what the \textbf{gradient descent} algorithm does.

Let's first see a general graph of the cost function. In general, it looks like
figure \ref{Cost_function.png}

\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{Res/Cost_function.png}
\caption{A generic graph of the cost function.}
\label{Cost_function.png}
\end{figure}

We see that $J$ is what we call a \textbf{convex function}, which means that it
is a function that was only one \textbf{local minimum} (or local maxima). This
property is very important if we want to apply the gradient descent algorithm.

In Gradient Descent, we initialize $b$ and $w$ randomly and take steps into the
direction that leads us to the lowest possible value of $J$. In order to do
that, we calculate the \textbf{gradient} (the derivatives in each direction) of
the function $J$ and take a step in the opposite direction of the gradient.

\begin{prop}
The gradient gives us the direction of the maximum increase of the a function.
\end{prop}

\begin{algorithm}[Gradient Descent]
Repeat \{ \nl
\t $w:=w-\alpha\parderiv[w]{J(w,b)}$ \nl
\t $b:=b-\alpha\parderiv[b]{J(w,b)}$ \nl
\}
\end{algorithm}

In the algorithm, $\alpha$ is what we call the \textbf{learning rate}. It's how
large we should step in the direction of the maximum decrease. If we take big
steps, we can go much faster to the global minumum, but we might not be so
accurated. On the other hand, we be take small steps, we can find the global
minum accuratedly, but achieve it much slower.

Gradient Descent is a general optimization algorithm and can be applied to any
convex function. So now we need to understand how to use it with logistic
regression.

After calculating the derivatives, we'll have:

\begin{algorithm}[Gradient Descent for Logistic Regression]
Repeat \{ \nl
\t$J=0;~dw=0;~db=0$ \nl
\t For $i=1\cdots m$:\nl
\t\t $\xii[z]{i}=w^{T}\xii[x]{i}+b$\nl
\t\t $\xii[a]{i}=\sigma(\xii[z]{i})$\nl
\t\t $J+=
-\sbracket{\xii[y]{i}\log\xii[a]{i}+(1-\xii[y]{i})\log(1-\xii[a]{i})}$\nl
\t\t $d\xii[z]{i}=\xii[a]{i}-\xii[y]{i}$\nl
\t\t $dw += \xii[x]{i}d\xii[z]{i}$\nl
\t\t $db += d\xii[z]{i}$\nl
\t $J/=m$\nl
\t $dw/=m;~db/=m$\nl
\t $w:=w-\alpha dw$ \nl
\t $b:=b-\alpha db$ \nl
\}
\end{algorithm}

This version of the algorithm uses a for loop to compute $J$, $dw$ and $db$. But
when implementing the code into Python or other language, we always try to
\textbf{vectorize} the code to make it faster.

\begin{algorithm}[Gradient Descent for Logistic Regression Vectorized]
Repeat \{ \nl
\t $Z=w^{T}X+b$ \nl
\t $A=\sigma(Z)$ \nl
\t $dZ = A - Y$ \nl
\t $dw = \dfrac{1}{m}XdZ^{T}$ \nl
\t $db = \dfrac{1}{m}\spsum{}{}dZ$ \nl
\t $w:=w-\alpha dw$ \nl
\t $b:=b-\alpha db$ \nl
\}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Basical Concepts}%
\label{cha:basical_concepts}

So let's start introducing some notation.

Whenever we use:
\[
    \xlayer[z]{i}
\]

we're talking about the $z$ values of the $i$-th \textit{layer} of the neural
network.

Thus, we have the following important equation:


\begin{equation}
\boxed{
    \xlayer[z]{i} = \xlayer[W]{i}\xlayer[a]{i-1} + \xlayer[b]{i}}
\label{forward-z}
\end{equation}

\begin{equation}
\boxed{
    \xlayer[a]{i} = \sigma(\xlayer[z]{i})}
\label{foward-activation}
\end{equation}

\section{Representation}%
\label{sec:representation}

Let's ilustrate a simple neural net:

\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{Res/nn_representation.png}
\label{nn_representation.png}
\end{figure}

This nn is divided into three parts:
\begin{itemize}
    \item The \textbf{input layer} is the layer were we enter the data, the
        features.  It's represented by a single vector $x$, which is a column of
        our inputs matrix $X$. Another way to denote the input is by
        $\xlayer[a]{0}$;
    \item The \textbf{output layer} is the layer were we get our answer. It can
        be one of more nodes and it's represented by a single vector $y$. It can
        be a binary vector, continuous vector, it could be even a codification
        of an image, sound, word or any codification just like the input;
    \item Finally, the \textbf{hidden layer} are all the nodes which are between
        the input and output, they are the intermediately computations that our
        neural net does. We don't need to stick to just one hidden layer, there
        could be many of them.
\end{itemize}

One more thing to keep in mind is that the neural network drawn in the picture
is called a \textit{two layer} neural network, because we don't count the input
layer as a ``real'' layer (it's the layer zero).

Each layer (real layer) will have \textbf{parameters} associated with them,
which we'll denote $\xlayer[W]{i}$ and $\xlayer[b]{i}$ for the $i$-th layer.

To understand what are node computes given an input, let's focus in the first
node of the hidden layer in the figure. We can see that the inputs $x_1,x_2,x_3$
are all passed to that node, which uses tham to output something to the node in
the output layer (or it could be outputed to the next hidden layer).

So basicaly a node is a logistic regression, it computes:
\[
    a = \sigma(Wx+b)
\]

But as we have many computations like this, we have to use indeces to denote the
first hidden layer and the first node of that layer. So the computations would
be described as following:

\[
\xlayer[z]{1}_1=\xlayer[W]{1}_1^{T}x+\xlayer[b]{1}_1
\]
\[
\xlayer[a]{1}_1=\sigma(\xlayer[z]{1}_1)
\]

The superscript means we're talking about the first (real) layer and the
subscript means we're talking about the first node of that layer. Therefore
$\xlayer[a]{l}_i$ is the \textbf{activation value} (output value) of the $i$-th
node in the $l$-th layer.

Of course, when coding a neural network, we don't compute each one of these
equations using a for loop, we vectorize in order to compute the whole vector
$\xlayer[z]{l}$ at once using equation \ref{forward-z}.

\paragraph{Vectorizing the input}%
\label{par:vectorizing_the_input}

Now, one more thing that we want is to be able to predict \textit{mulpible}
inputs at once. Indeed we can do that with some vectorization. Let's see how.

So let's recall that
\[
X = \begin{bmatrix}
    | & | &  & | \\
    \xii[x]{1} & \xii[x]{2} & \cdots & \xii[x]{m} \\
    | & | &  & | \\
    \end{bmatrix}
\]
and let's define a matrix $\xlayer[Z]{l}$ in which the $j$-th column is the
vector $z^{[l](j)}$ (i.e., the value before the activation function of the layer
$l$ when applied to the input $j$).

\[
\xlayer[Z]{l} = \begin{bmatrix}
    | & | &  & | \\
    z^{[l](1)} & z^{[l](2)} & \cdots & z^{[l](m)} \\
    | & | &  & | \\
    \end{bmatrix}
\]

and also define the matrix $\xlayer[A]{l}$, which is $\sigma(\xlayer[Z]{l})$:

\[
\xlayer[A]{l} = \begin{bmatrix}
    | & | &  & | \\
    a^{[l](1)} & a^{[l](2)} & \cdots & a^{[l](m)} \\
    | & | &  & | \\
    \end{bmatrix}
\]

We can see that each row of matrix $A$ tell us the activation value for a
particular node of that layer for each input. For instance, the value
$A^{[l]}_{i,j}$ is the value of the $i$-th neuron in the $l$-th layer of the
neural network when we input the $j$-th input.

Given these matrices, we can train over all inputs using the vectorized formula:

\begin{equation}
    \boxed{\xlayer[Z]{l}=\xlayer[W]{l}\xlayer[A]{l-1}+\xlayer[b]{l}}
\end{equation}

\begin{equation}
    \boxed{\xlayer[A]{l}=\sigma(\xlayer[Z]{l})}
\end{equation}

and recall that $X=\xlayer[A]{0}$.

\section{Activation Functions}%
\label{sec:activation_functions}

The \textbf{activation function} is the function $g$ applied to $z$. We've
been currently using the \textit{sigmoid function} $\sigma$, but there are some
other functions that can be used and can significatly change our results and
performances.

Another function one can use is the $\tanh$ function, given by the formula:
\[
    \tanh(z) = \dfrac{e^{z}-e^{-z}}{e^{z}+e^{-z}}
\]

which is a shifted version of the sigmoid function. We have $\tanh(0)=0$ instead
of $\sigma(0)=0.5$.

It turns out that the $\tanh$ function frequently works better than the sigmoid,
because the values are between $-1$ and $1$ instead of $0$ and $1$ and thus the
average values tend to be closer to zero.

The exception is for the output layer, where it's good to have a number between
$0$ and $1$ in many cases, so we could keep using the sigmoid function there and
use the tanh on the hidden layers.

One of the down sides of these two functions is that when $z$ is very large, the
gradient is very small, because they're very flat for large (positive or
negative) values of $z$.

That's why many times we see the \textbf{ReLU} (rectified linear unit) function
being used:

\[
    ReLU(z) = \dmax(0,z)
\]

Indeed the ReLU is the most used activation in practice and should be the first
choise. One of the only down sides of it is that the derivative for values lower
than zero is zero, but it works fine in practice.

There's modified version of the ReLU called \textbf{Leaky ReLU}, which is given
by:

\[
    LReLU(z) = \dmax(0.01z,z)
\]

It's just like the ReLU function, but it's not zero for any negative value of
$z$ and, thus, the derivative is not null on those points. Actually, indeed
there no reason for the $0.01$, it could be any small value and we can turn that
into a hyperparameter of our algorithm.

\paragraph{Why activation functions}%
\label{par:why_activation_functions}

Now that we've seen so many activation functions we might want to understand why
do we even need them. And most importantly, why do we need they to be
non-linear.

We could try using $a=g(z)=z$ (i.e., doing nothing). It turns out that if we
have just the identity function, we can't express \textit{complex} (non-linear)
decision boundaries.

We'll not prove it here, but if we just use a linear function in all hidden
nodes and a sigmoid, it is equivalent to a standard logistic regression in terns
of what it can express.

\begin{align*}
\xlayer[a]{1}&=\xlayer[w]{1}x+\xlayer[b]{1} \\ \\
\xlayer[a]{2}&=\xlayer[w]{2}\xlayer[a]{1}+\xlayer[b]{2} \\
             &=\xlayer[w]{2}\paren{\xlayer[a]{1}=\xlayer[w]{1}x+\xlayer[b]{1}}+
             \xlayer[b]{2} \\
             &=\paren{\xlayer[w]{2}\xlayer[w]{1}}x+
             \paren{\xlayer[w]{2}\xlayer[b]{1}+\xlayer[b]{2}}=\\
             &=w'x+b'
\end{align*}

Above we can see that from one layer to the next we still have a linear
equation (a composition of two linear functions is a linear function).

\section{Backpropagation}%
\label{sec:backpropagation}

Now we're going to understand how to find the optimal values for $W$ and $b$
using backpropagation.

The first step in this direction is calculating the derivative of the activation
functions. So let's to it.

\paragraph{Sigmoid}%
\label{par:sigmoid}

\[
    g(z)=\dfrac{1}{1+e^{-z}}
\]
\begin{align*}
    \deriv[z]{g(z)}&=\dfrac{1}{1+e^{-z}}\paren{1-\dfrac{1}{1+e^{-z}}} \\
                   &= g(z)(1-g(z))
\end{align*}

\paragraph{Tanh}%
\label{par:tanh}

\[
    g(z)=\tanh(z)
\]
\begin{align*}
    \deriv[z]{g(z)}&=1-\paren{\tanh(z)}^2 \\
                   &=1-g(z)^2
\end{align*}

\paragraph{ReLU}%
\label{par:relu}

\[
    g(z)=\dmax(0,z)
\]
\[
    \deriv[z]{g(z)}=\begin{cases}
        0, &\text{ if }z<0 \\
        1, &\text{ if }z>0 \\
        \text{undef}, &\text{ if }z=0
    \end{cases}
\]

\paragraph{Leaky ReLU}%
\label{par:leaky_relu}

\[
    g(z)=\dmax(0.01z,z)
\]
\[
    \deriv[z]{g(z)}=\begin{cases}
        0.01, &\text{ if }z<0 \\
        1, &\text{ if }z>0 \\
        \text{undef}, &\text{ if }z=0
    \end{cases}
\]

Now let's understand how to do \textbf{gradient descent}.

Let's denote $L$ the number of layers of our network. Them, the last value
derivative (the first we're going to compute) is given by the formula:

\begin{equation}
    \xlayer[dz]{L}=\xlayer[a]{L}-y
    \label{last-dz}
\end{equation}

The values of $dW$ and $db$, for any layer are given by the formulas:

\begin{align}
    \xlayer[dW]{l}&=\xlayer[dz]{l}\xlayer[a]{l-1}^{T} \\
    \xlayer[db]{l}&=\xlayer[dz]{l}
    \label{dW-db}
\end{align}

The other values of $dz$ for any layer other than the output layer are given by
the formulas:

\begin{equation}
    \xlayer[dz]{l}=\xlayer[W]{l+1}^{T}\xlayer[dz]{l+1}*
    \xlayer[g]{l}'\paren{\xlayer[z]{l}}
    \label{dz-equation}
\end{equation}

where $*$ denotes que \textit{element-wise} product.

There's also a vectorized way of computing this for all the training examples at
once.

\begin{align}
    \xlayer[dZ]{L}&=\xlayer[A]{L}-Y \\
    \xlayer[dW]{l}&=\dfrac{1}{m}\xlayer[dZ]{l}\xlayer[A]{l-1}^{T} \\
    \xlayer[db]{l}&=\dfrac{1}{m}\text{sum}\paren{\xlayer[dZ]{l}, \text{axis=1})}
        \\
    \xlayer[dZ]{l}&=\xlayer[W]{l+1}^{T}\xlayer[dZ]{l+1}*
    \xlayer[g]{l}'\paren{\xlayer[Z]{l}}
\end{align}

\section{Random Initialization}%
\label{sec:random_initialization}

Finanlly, we need to initialize the parameters of the neural network. It turns
out that initializing everything as zero or everything as the same number, then
all nodes of a layer will be exactly the same even with many iterations of
gradient descent.

For the values of $b$, we can initialize with zero, but for $W$, we prefer to
initialize with \textit{small} random numbers. We like the numbers to be small
though because if we're using $\tanh$ or $\sigma$, big numbers will have a very
plat derivative and, thus, the algorithm will learn slower.

\section{Notation for deep neural networks}%
\label{sec:notation_for_deep_neural_networks}

We'll use $L$ to describe the number of layers the neural network have
(remember, the input layer is not a real layer).

$\xlayer[n]{l}$ denotes the number of nodes on layer $l$.

$\xlayer[a]{l}$ denotes the activation values on layer $l$.

$\xlayer[g]{l}$ denotes the activation function used on layer $l$.

Also, the dimensions of $\xlayer[W]{l}$ and $\xlayer[b]{l}$ are
$\pair{\xlayer[n]{l}}{\xlayer[n]{l-1}}$ and $\pair{\xlayer[n]{l}}{1}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Practical Aspects of Deep Learning}%
\label{cha:practical_aspects_of_deep_learning}

In this chapter we're going to focus on the practical aspects of deep learning,
like how to choose the hyperparameters, how to make the code faster, how to
apply regularization and others.

The first thing to understand is that, in practical, we divide our data set
into three:
\begin{itemize}
    \item \textit{Training set}, in which we're going to train the model;
    \item \textit{Hold-out cross validation set (or dev set)}, in which we're
        going to evaluate our model and choose hyperparameters;
    \item \textit{Test set}, in which we're going to evaluate our final model,
        with the hyperparameters setted.
\end{itemize}

In the previous era of Machine Learning, we would use $70\%$ for the
training/dev sets and $30\%$ for testing, or $60/20/20\%$. But now in the big
data era, we can use much small fractions for the testing and hold-out sets
(like $1\%$ or even less, since this can be equivalent to about $10000$
examples).

Another important thing to keep in mind is that the training set and test set
must have the \textit{same distribution} of that. For instance, if we train on
images of cats coming from the web, than we should not test on images of cats
coming from mobile cameras.

\section{Regularization}%
\label{sec:regularization}

We've seen that the cost function of our neural network is a function $J$ such
that:

\[
    J\paren{\xlayer[W]{1},\xlayer[b]{1},\cdots,\xlayer[W]{L},\xlayer[b]{L}}=
    \dfrac{1}{m}\spsum{i=1}{m}\mathcal{L}(\hat{\xii[y]{i}},\xii[y]{i})
\]

\textbf{Regularizing} a neural net model means adding a penality when the model
makes $\xlayer[W]{i}$ bigger. This helps the model to prevent overfitting.

Thus, we have:
\[
    J\paren{\xlayer[W]{1},\xlayer[b]{1},\cdots,\xlayer[W]{L},\xlayer[b]{L}}=
    \dfrac{1}{m}\spsum{i=1}{m}\mathcal{L}(\hat{\xii[y]{i}},\xii[y]{i})
    + \dfrac{\lambda}{2m}\spsum{l=1}{L}\norm{\xlayer[W]{l}}^2
\]

Where

\[
\norm{\xlayer[W]{l}}^2=\spsum{i=1}{\xlayer[n]{l}}\spsum{j=1}{\xlayer[n]{l-1}}
\paren{\xlayer[W]{l}_{i,j}}^2
\]

is called the \textit{Frobenius norm} (sometimes we write
$\norm{\xlayer[W]{l}}^2_F$).

Here $\lambda$ is called the \textbf{regularization factor} and the bigger it
is, the more we penalise the model for having bigger $\xlayer[W]{i}$. \jump

Another kind of regularization is called \textbf{dropout regularization}. In
this kind of regularization, we set a small probability of removing a node at
all and have a new smaller network.

One way to implement it is called \textit{inverted dropout}.

\begin{lstlisting}[language=python]
l = 3
keep-prob = 0.8
d3 = np.random.rand(a3.shape[0]. a3.shape[1]) < keep-prob
a3 = np.multiply(a3, d3)
a3 /= keep-prob
\end{lstlisting}

The last line is used to keep the espected value of $a$ the same, so we keep the
same scale of $z$ when doing $z=wa+b$.

When using dropout, though, we \textit{don't apply it when using the neural net
to predict values}.

To explain intuitively why dropout works, we need to think about the nodes
connected to a particular node. If the NN just give importance to one of the
those nodes, then it might be dropped and the model will perform poorly.
Therefore, to work well, the model needs to spread the weights into all the
neurons, shirking them.

There are other ways to perform regularization.

\textbf{Data augmentation} is when we change the data to get more data. For
instance, if we have a image dataset, we can flip the images, reduce saturation,
blur the image, and do many other things to get more images to train the make
our mode better.

If we have a sound dataset, we can make the sound loud, or add noise, supression
and so on.

\textbf{Early stopping} is when we stop iterating our model using the error in
the dev set. While the error in the train error one decreases, the error in the
dev set tends to decrease for a while and than starts incrasing. What early
stopping does is stop iterating the model when the error in the dev set starts
increasing.

\section{Optimizing the Problem}%
\label{sec:optimizing_the_problem}

\subsection{Normalizing input}%
\label{sub:normalizing_input}

Normalizing the input is very important because it garantees to us that the
input data will have zero mean and variance one. To perform normalization, we
calculate:
\[
\mu=\dfrac{1}{m}\spsum{i=1}{m}\xii[x]{i}
\]
and
\[
\sigma^2=\dfrac{1}{m}\spsum{i=1}{m}\xii[x]{i}^2
\]

where $\xii[x]{i}^2$ mean elementwise power.

Please note that these values are calculated using the \textit{training set}.
And them we'll apply the below formula to \textit{every} input that we'll feed
into the network (it can come from the training, dev or test set).

\[
x:=\dfrac{x-\mu}{\sigma}
\]

Normalizing the input is important because it garantees that Gradient Descent
will converge much faster, since $J$ will have a more circular bow shape, since
of a elliptical one.

\subsection{Vanishing / Exploding Gradients}%
\label{sub:vanishing_exploding_gradients}

In this section we're going to cover a problem that we can face when traning
neural networks. Sometimes, the gradients become too small or too big and we
can't training anything at all.

To understand that, suppose we have a very deep neural network. It's not so hard
to imagine that as computations are performed upon computations, we can get very
large or very small numbers for values like the activation function.

For instance, suppose all $\xlayer[W]{l}$ have values of $1.5$ then if we don't
use an activation function, our final output will be something proportional to
$\xlayer[W]{1}\xlayer[W]{2}\cdots\xlayer[W]{L}$, which will me proportional to
$1.5^{L}$. If $L$ is very big, this can be huge.

If we changed $1.5$ by $0.5$, then we would have amoust zero.

To solve (parcially) this problem, we need to have a careful initialization of
our weights.

Let's just consider a single neuron. If we ignore $b$, then we have
\[
z = w_1x_1+w_2x_2+\cdots+w_nx_n
\]

The later $n$ is, the smaller we want $w_i$ to be, because we want $z$ to be
approximately in the range $\sbracket{-1,1}$. One thing that we would wish is to
have
\[
    \var{w_i}=\dfrac{1}{n}
\]

So we can initialize it like:
\[
    \xlayer[W]{i}=\text{np.random.randn(shape)}\cdot\text{np.sqrt}\paren{\dfrac{1}{\xlayer[n]{l-1}}}
\]

It turns out that if we are using ReLU, it's better to have a variance of
$\dfrac{2}{n}$, so we can replace the one by $2$ in the sqrt.

The variatian with $1$ is called the \textit{Xavier initialization}, but we can
use another variatian in which we multiply by:
\[
\sqrt{\dfrac{2}{\xlayer[n]{l-1}+\xlayer[n]{l}}}
\]

These factors could all be used as a base model, but we can really tune this as
a hyperparameter if we want, although it's not a very important hyperparameter.

\subsection{Gradient Checking}%
\label{sub:gradient_checking}

A very common technique when using Gradient Descent is to use \textbf{Gradient
Checking} to make sure our gradient computations are been performed right. It
will not go to the final model, but we use it to make sure we've coded
everything right.

To do it, basicaly we approximate the gradients numericaly and compare with the
gradient we're computing using the formulas. Of course we don't do that when we
want to predict any value or use our model in the real world because we're
computing the gradient two times (and computing it numericaly is not efficient).
We just use this technique when coding the backpropagation to make sure we're
doing it right.

To approximate the gradient, it's very since, we just use:
\[
    g(\theta)\approx\dfrac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}
\]
which is called \textit{two-sided approximation}.

So the first thing in order to use gradient checking is to reshape our
parameters $\xlayer[W]{1},\xlayer[b]{1},\cdots,\xlayer[W]{L},\xlayer[b]{L}$ into
just one big vector $\theta$. And it's derivative will be $d\theta$

Now we're going to compute $d\theta_{\approx}$ which can be computed
doing:
\[
    d\theta_{\approx i}=
    \dfrac{J(\theta_1,\theta_2,\cdots,\theta_i+\epsilon,\cdots,\theta_k)-
              J(\theta_1,\theta_2,\cdots,\theta_i-\epsilon,\cdots,\theta_k)}
              {2\epsilon}
\]

For each value of $i$.

And what we want is
\[
d\theta_{\approx}\approx d\theta
\]

To now that, we can calculate:
\[
    \dfrac{\norm{d\theta_{\approx}-d\theta}}
    {\norm{d\theta_{\approx}}+\norm{d\theta}} < \delta
\]
for a small value of $\delta$. It could be $10^{-7}$ to make sure it's really
correct or $10^{-5}$. If the difference is greater than something like $10^{-3}$
than probability we have a bug somewhere.

\section{Optimization Algorithms}%
\label{sec:optimization_algorithms}

In this section, we're going to view many algorithms that allow us to train much
faster.

\subsection{Mini-batch Gradient Descent}%
\label{sub:mini_batch_gradient_descent}

What we've been doing in terms of gradient descent so far is to process the
whole training set to take a step in the opposite direction of the gradient.

If $m$, our number of training exemples, is very large (which is normal in Big
Data applications), then a single gradient step is very expensive.

One way to optimize that is to use what's called \textbf{mini-batches}.
Basically we split our training set into multiple training sets (if say $1000$
examples) and we run each step of gradient descent using one of these sets.

That's what we call an \textbf{epoch} of gradient descent (a pass of
forward-prop, cost computation, back-prop and parameters update).

Of course, by doing this, we're not optimizing the original $J$ function (which
is computed for every training example). However, we'll get a low cost model at
the end.

If we plot the cost of $J$ at each iteration (epoch), we'll not have that
classical curve in which $J$ decreases at each iteration. The curve will be a
little noisy, increasing and decreasing. But in the long run, it'll converge to
some low value. And not just that, it does it much faster than the original
gradient descent (which is also called \textbf{batch gradient descent}).

One important concept to mention is the size of the mini-batches. If the size is
one, than we call the algorithm \textbf{stochastic gradient descent}. This will
make our epochs really fast, but we'll lead to an algorithm with a much higher
cost than we could get using greater batches. Stochastic gradient descent will
be \textit{around} the minimum optima of $J$. The greater our mini-batches are,
the more closer the that mininum optima we'll lead. That's the trade-off between
speed and precision.

We can also train with mini-batch until the cost is very close to the mininum
optima and then change to batch gradient descent and run some more epochs to
make sure that our algorithm is really precise.

In practical, because how the computer memory and vectorization works, people
tend to use powers of two for the size of the mini-batches. So that's a thing to
keep in mind. Common values are $2^{6}$ up to $2^{9}$.

\subsection{Exponentially weighted averages}%
\label{sub:exponentially_weighted_averages}

Now we're going to cover other optimization algorithms that are better than
gradient descent. But to understand them, we need to talk about a concept of
statistics which is called \textbf{exponentially weighted (moving) averages}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
